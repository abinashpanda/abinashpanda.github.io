<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: MachineLearning | Pythonic Musings]]></title>
  <link href="http://abinashpanda.github.io/blog/categories/machinelearning/atom.xml" rel="self"/>
  <link href="http://abinashpanda.github.io/"/>
  <updated>2013-12-29T09:58:15+05:30</updated>
  <id>http://abinashpanda.github.io/</id>
  <author>
    <name><![CDATA[Abinash Panda]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Introduction to Machine Learning]]></title>
    <link href="http://abinashpanda.github.io/blog/2013/12/28/introduction-to-machine-learning/"/>
    <updated>2013-12-28T16:28:00+05:30</updated>
    <id>http://abinashpanda.github.io/blog/2013/12/28/introduction-to-machine-learning</id>
    <content type="html"><![CDATA[<p>TeX: {
  extensions: [“AMSmath.js”]
}</p>

<p>Machine learning, a branch of artificial intelligence, concerns the construction and study of systems that can learn from data. With a deluge of machine learning sources both online and offline, a newcomer in this field would simply get stranded due to indecisiveneww. This post is for all <em>Machine Learning Enthusiasts</em> who are not able to find a way to understand <em>Machine Learning (ML)</em>.</p>

<p>This tutorial doesn’t require you to have a good deal of understanding of optimizations, linear algebra or probability. It is about learning basic concepts of Machine Learning and coding it. I would be using a python library <a href="http://scikit-learn.org/stable/">scikit-learn</a> for various <em>ML</em> applications.</p>

<p>Let’s start with a very simple Machine Learning algorithm <strong>Linear Regression</strong>.</p>

<h2 id="linear-regression">Linear Regression</h2>

<p>Linear Regression is an approach to the model the relationship between a <em>scalar dependent variable y</em> and <em>one or more indenpendent variable X</em>.</p>

<script type="math/tex; mode=display">
\begin{align}
y = \left [ \begin{array}{c}
      y^1 \\
      \vdots \\
      y^n
    \end{array}\right]    \hspace{10 mm}
X = \left[ \begin{array}{cc}
     X^1_1 \dots X^1_m \\
     \vdots \ddots \vdots \\
     X^n_1 \dots X^n_m
     \end{array} \right]
\end{align}
</script>

<p><em>n = number of samples</em><br />
<em>m = number of features</em></p>

<p>A linear regression model assumes that the relationship between the dependent variable $y_i$ and independent variable $X_i$.</p>

<script type="math/tex; mode=display">
\begin{align}
y^i = a_0 + a_1*X^i_1 + a_2*X^i_2 + \dots + a_m*X^i_m
\end{align}
</script>

<script type="math/tex; mode=display">
\begin{align}
y = \left[ \begin{array}{cc}1 \hspace{3 mm} X^1_1 \hspace{3 mm} \dots \hspace{3 mm} X^1_m \\
		   1 \hspace{3 mm} X^2_1 \hspace{3 mm} \dots \hspace{3 mm}  X^2_m \\
           \vdots \hspace{3 mm} \ddots \hspace{3 mm} \vdots \\
           1 \hspace{3 mm} X^n_1 \hspace{3 mm} \dots \hspace{3 mm} X^n_m
           \end{array} \right]
    \left[ \begin{array}{c} a_0\\
           a_1\\
           \vdots \\
           a_m
           \end{array} \right]^T
\end{align}
</script>

<p><em>a<sub>0</sub>, a<sub>1</sub>, …. , a<sub>m</sub></em> are some constants.</p>

<h4 id="linear-regression-with-one-variable-univariate">Linear Regression with One Variable (Univariate)</h4>

<p>First we start with modelling a hypothesis $h_\theta(X)$.</p>

<script type="math/tex; mode=display">
\begin{align}
h_\theta(X) = \theta_0 + \theta_1*X
\end{align}
</script>

<p>The objective of linear regression is to correctly estimate the values of <script type="math/tex">\theta_0</script> and <script type="math/tex">\theta_1</script> such that <script type="math/tex">h_\theta(X)</script> approximates to <script type="math/tex">y</script>. But how to do that?. For this we define a cost function or error function <script type="math/tex">J(\theta)</script> as:</p>

<script type="math/tex; mode=display">
\begin{align}
J(\theta) =  \frac{1}{2n}\sum_{i=1}^n (h_\theta(X^i) - y^i)
\end{align}
</script>

<p>Linear Regression models are often fitted using <em>least squares</em> approach i.e. by minimizing squared error function (or by minimizing  a <em>penalized version of the squares error function</em>). For minimizing the <em>error function</em> we use the <em><a href="http://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent Algorithm</a></em>. This method is based on the observation that if a function <script type="math/tex">f(x)</script> is <em>defined</em> and <em>differentiable</em> in the neighborhood of a point <script type="math/tex">\beta</script>, then <script type="math/tex">f(x)</script> decreases <em>fastest</em> if one goes from <script type="math/tex">\beta</script> in the direction of <em>negative gradient of</em> <script type="math/tex">f(x)</script> at <script type="math/tex">\beta</script>. So, we can find the minima by updating the value of <script type="math/tex">\beta</script> as:</p>

<script type="math/tex; mode=display">
\begin{align}
\beta := \beta - \alpha\nabla f(\beta)
\end{align}
</script>

<p>Where <script type="math/tex">\alpha</script> is the step size.<br />
Using the above concept, we can find the values of <script type="math/tex">\theta_0</script> and <script type="math/tex">\theta_1</script> as:</p>

<script type="math/tex; mode=display">
\begin{align}
\theta_0 := \theta_0 - \frac{\partial J(\theta)}{\partial \theta_0}\\
\theta_1 := \theta_1 - \frac{\partial J(\theta)}{\partial \theta_1}
\end{align}
</script>

<p>Replacing the values of <script type="math/tex">\frac{\partial J(\theta)}{\partial \theta_i}</script> as </p>

<script type="math/tex; mode=display">
\begin{align}
\frac{\partial J(\theta)}{\partial \theta_i} = \frac{1}{n}\sum_{j=1}^n(h_\theta(X^j) - y^j)X_i
\end{align}
</script>

<p>We can have a general formula for finding optimal value for any <script type="math/tex">\theta_i</script> as:</p>

<script type="math/tex; mode=display">
\begin{align}
\theta_i := \theta_i - \frac{1}{n}\sum_{j=1}^n(h_\theta(X^j) - y^j)X_i
\end{align}
</script>

<p>Phew!!!. A lot of mathematics</p>
]]></content>
  </entry>
  
</feed>
