<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: MachineLearning | Pythonic Musings]]></title>
  <link href="http://abinashpanda.github.io/blog/categories/machinelearning/atom.xml" rel="self"/>
  <link href="http://abinashpanda.github.io/"/>
  <updated>2013-12-30T23:13:00+05:30</updated>
  <id>http://abinashpanda.github.io/</id>
  <author>
    <name><![CDATA[Abinash Panda]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Introduction to Machine Learning]]></title>
    <link href="http://abinashpanda.github.io/blog/2013/12/28/introduction-to-machine-learning/"/>
    <updated>2013-12-28T16:28:00+05:30</updated>
    <id>http://abinashpanda.github.io/blog/2013/12/28/introduction-to-machine-learning</id>
    <content type="html"><![CDATA[<p>Machine learning, a branch of artificial intelligence, concerns the construction and study of systems that can learn from data. With a deluge of machine learning sources both online and offline, a newcomer in this field would simply get stranded due to indecisiveneww. This post is for all <em>Machine Learning Enthusiasts</em> who are not able to find a way to understand <em>Machine Learning (ML)</em>.</p>

<p>This tutorial doesn’t require you to have a good deal of understanding of optimizations, linear algebra or probability. It is about learning basic concepts of Machine Learning and coding it. I would be using a python library <a href="http://scikit-learn.org/stable/">scikit-learn</a> for various <em>ML</em> applications.</p>

<p>Let’s start with a very simple Machine Learning algorithm <strong>Linear Regression</strong>.</p>

<h2 id="linear-regression">Linear Regression</h2>

<p>Linear Regression is an approach to the model the relationship between a <em>scalar dependent variable y</em> and <em>one or more indenpendent variable X</em>.</p>

<script type="math/tex; mode=display">
\begin{align}
y = \left [ \begin{array}{c}
      y^1 \\
      \vdots \\
      y^n
    \end{array}\right]    \hspace{10 mm}
X = \left[ \begin{array}{cc}
     X^1_1 \dots X^1_m \\
     \vdots \ddots \vdots \\
     X^n_1 \dots X^n_m
     \end{array} \right]
\end{align}
</script>

<p><em>n = number of samples</em><br />
<em>m = number of features</em></p>

<p>A linear regression model assumes that the relationship between the dependent variable $y_i$ and independent variable $X_i$.</p>

<script type="math/tex; mode=display">
\begin{align}
y^i = a_0 + a_1*X^i_1 + a_2*X^i_2 + \dots + a_m*X^i_m
\end{align}
</script>

<script type="math/tex; mode=display">
\begin{align}
y = \left[ \begin{array}{cc}1 \hspace{3 mm} X^1_1 \hspace{3 mm} \dots \hspace{3 mm} X^1_m \\
		   1 \hspace{3 mm} X^2_1 \hspace{3 mm} \dots \hspace{3 mm}  X^2_m \\
           \vdots \hspace{3 mm} \ddots \hspace{3 mm} \vdots \\
           1 \hspace{3 mm} X^n_1 \hspace{3 mm} \dots \hspace{3 mm} X^n_m
           \end{array} \right]
    \left[ \begin{array}{c} a_0\\
           a_1\\
           \vdots \\
           a_m
           \end{array} \right]^T
\end{align}
</script>

<p><em>a<sub>0</sub>, a<sub>1</sub>, …. , a<sub>m</sub></em> are some constants.</p>

<h4 id="linear-regression-with-one-variable-univariate">Linear Regression with One Variable (Univariate)</h4>

<p>First we start with modelling a hypothesis $h_\theta(X)$.</p>

<script type="math/tex; mode=display">
\begin{align}
h_\theta(X) = \theta_0 + \theta_1*X
\end{align}
</script>

<p>The objective of linear regression is to correctly estimate the values of <script type="math/tex">\theta_0</script> and <script type="math/tex">\theta_1</script> such that <script type="math/tex">h_\theta(X)</script> approximates to <script type="math/tex">y</script>. But how to do that?. For this we define a cost function or error function <script type="math/tex">J(\theta)</script> as:</p>

<script type="math/tex; mode=display">
\begin{align}
J(\theta) =  \frac{1}{2n}\sum_{i=1}^n (h_\theta(X^i) - y^i)
\end{align}
</script>

<p>Linear Regression models are often fitted using <em>least squares</em> approach i.e. by minimizing squared error function (or by minimizing  a <em>penalized version of the squares error function</em>). For minimizing the <em>error function</em> we use the <em><a href="http://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent Algorithm</a></em>. This method is based on the observation that if a function <script type="math/tex">f(x)</script> is <em>defined</em> and <em>differentiable</em> in the neighborhood of a point <script type="math/tex">\beta</script>, then <script type="math/tex">f(x)</script> decreases <em>fastest</em> if one goes from <script type="math/tex">\beta</script> in the direction of <em>negative gradient of</em> <script type="math/tex">f(x)</script> at <script type="math/tex">\beta</script>. So, we can find the minima by updating the value of <script type="math/tex">\beta</script> as:</p>

<script type="math/tex; mode=display">
\begin{align}
\beta := \beta - \alpha\nabla f(\beta)
\end{align}
</script>

<p>Where <script type="math/tex">\alpha</script> is the step size.<br />
Using the above concept, we can find the values of <script type="math/tex">\theta_0</script> and <script type="math/tex">\theta_1</script> as:</p>

<script type="math/tex; mode=display">
\begin{align}
\theta_0 := \theta_0 - \alpha\frac{\partial J(\theta)}{\partial \theta_0}\\
\theta_1 := \theta_1 - \alpha\frac{\partial J(\theta)}{\partial \theta_1}
\end{align}
</script>

<p>Here <script type="math/tex">\alpha</script> is called as the <em>learning rate</em>.  <br />
Replacing the values of <script type="math/tex">\frac{\partial J(\theta)}{\partial \theta_i}</script> as </p>

<script type="math/tex; mode=display">
\begin{align}
\frac{\partial J(\theta)}{\partial \theta_i} = \frac{1}{n}\sum_{j=1}^n(h_\theta(X^j) - y^j)X_i^j
\end{align}
</script>

<p>We can have a general formula for finding optimal value for any <script type="math/tex">\theta_i</script> as:</p>

<script type="math/tex; mode=display">
\begin{align}
\theta_i := \theta_i - \alpha\frac{1}{n}\sum_{j=1}^n(h_\theta(X^j) - y^j)X_i^j
\end{align}
</script>

<p>Phew!!!. A lot of mathematics, right?. But where is the code?.</p>

<p>Let’s get our hands on some coding. For this tutorial I would be going to use <a href="http://scikit-learn.org/stable/">scikit-learn</a> for <em>machine learning</em> and <a href="http://matplotlib.org/">matplotlib</a> for <em>plotting</em>.</p>

<p>Suppose, for a hypothetical city <em>FooCity</em>, population in 10,000s and profit in $10,000 are <a href="/downloads/example1.txt">available</a>. We want to predict price of a house of particular size. </p>

<p>```python load_data
import numpy as np
import matplotlib.pyplot as plt</p>

<p>input_file = open(‘example1.txt’)
lines = input_file.readlines()</p>

<p>X = [map(float, line.strip().split(‘,’))[0] for line in lines]
#X : size of house
X = np.array(X)
#converting X from a list to array
X = X.reshape(X.shape[0], 1)
#reshaping the X from size(97, ) to (97, 1)</p>

<p>y = [map(float, line.strip().split(‘,’))[1] for line in lines]
#y : price of house
y = np.array(y)
#converting y from a list to array
y = y.reshape(y.shape[0], 1)
#reshaping the y from size(97, ) to (97, 1)</p>

<p>plt.plot(X, y, ‘r+’, label=’Input Data’)
#plotting house size vs house price
plt.ylabel(‘Profit in $10,000s’)
plt.xlabel(‘Population of City in 10,000s’)
plt.show()
```</p>

<p><img src="/images/figure_1.png"></p>

<p>It is visible from the plot that Population and Profit are varying linearly, so we can apply linear regression and predict profit for a given population.  <br />
For performing <em>Linear Regression</em> we have to use <code>LinearRegression</code> class available in <code>sklearn.linear_model</code>.</p>

<p>```python linear_regression
from sklearn.linear_model import LinearRegression</p>

<p>clf = LinearRegression()
clf.fit(X, y)
#linear regression using scikit-learn is very simple.
#just call the fit method with X, y
```
We can now predict the value of Profit for any Population(such as 15.12*10000) as <code>clf.predict(15.12)</code>.</p>

<p><code>python plot
x_ = np.linspace(np.min(X), np.max(X), 100).reshape(100, 1)
#x : array with 100 equally spaced elements starting with 
#min value of X upto max value of X
y_ = clf.predict(x_)
plt.plot(x_, y_, 'b', label='Predicted values')
plt.legend(loc='best')
</code> 
<img src="/images/figure_2.png"></p>

<p>Next, we would be going for <em>Multivariate Linear Regression</em>.</p>
]]></content>
  </entry>
  
</feed>
